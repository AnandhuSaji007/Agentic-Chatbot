{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb44cef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import ArxivQueryRun,WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper,ArxivAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173a13ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_wrapper_arxiv=ArxivAPIWrapper(top_k_results=2,doc_content_chars_max=500)\n",
    "arxiv=ArxivQueryRun(api_wrapper=api_wrapper_arxiv,description=\"Query arxiv papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14a0f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_wrapper_wiki=WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=500)\n",
    "wiki=WikipediaQueryRun(api_wrapper=api_wrapper_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f1ef93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"]=os.getenv(\"TAVILY_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "362a5126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WIIS\\AppData\\Local\\Temp\\ipykernel_3772\\1016542888.py:2: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tavily =TavilySearchResults()\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "tavily =TavilySearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "195e8393",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[arxiv,wiki,tavily]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2da66b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm=ChatGroq(model=\"qwen/qwen3-32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7c6a5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<think>\\nOkay, the user asked Who are you? I need to answer according to the instructions. First, I should clearly state my name, Qwen, and that I'm a large-scale language model developed by Alibaba Cloud's Tongyi Lab. Then, mention my capabilities, like answering questions, creating text such as stories, official documents, emails, and scripts, logical reasoning, programming, and more. Highlight that I support multiple languages. Finally, invite the user to ask questions or request assistance. Keep it natural, friendly, concise, and avoid markdown formatting. Let me put that together.\\n</think>\\n\\nHello! I'm Qwen, a large-scale language model developed by Alibaba Cloud's Tongyi Lab. I can help you with answering questions, writing stories, official documents, emails, scripts, logical reasoning, programming, and more. I support multiple languages and aim to provide accurate and helpful responses. How can I assist you today? Feel free to ask anything! ðŸ˜Š\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 198, 'prompt_tokens': 12, 'total_tokens': 210, 'completion_time': 0.462356164, 'prompt_time': 0.007022087, 'queue_time': 0.051129413, 'total_time': 0.469378251}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--3d71655a-dd8c-4163-b18d-c9967e564c71-0', usage_metadata={'input_tokens': 12, 'output_tokens': 198, 'total_tokens': 210})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "656dc4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools=llm.bind_tools(tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98dd74d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking for recent news in AI. I need to figure out which tool to use here. Let me check the available functions.\\n\\nFirst, there\\'s the arxiv function, which is for querying arXiv papers. That\\'s more for academic research, not news. Then there\\'s Wikipedia, which is good for general information but might not have the latest updates. The third option is tavily_search_results_json, which is a search engine optimized for current events. Since the user is asking about recent news, the tavily search seems appropriate. \\n\\nI should make sure that the query is properly formatted. The function requires a \"query\" parameter. The user\\'s question is straightforward, so the search query should be something like \"recent news in AI\". I\\'ll use that as the argument for the tavily_search function. No need to check other functions here because the focus is on current events. Alright, let\\'s go with that.\\n', 'tool_calls': [{'id': '5x5p639y1', 'function': {'arguments': '{\"query\":\"recent news in AI\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 223, 'prompt_tokens': 354, 'total_tokens': 577, 'completion_time': 0.582958673, 'prompt_time': 0.023372615, 'queue_time': 0.052681654, 'total_time': 0.606331288}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--fb8fd413-e78d-4bf1-8c09-c8191a5a1085-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'recent news in AI'}, 'id': '5x5p639y1', 'type': 'tool_call'}], usage_metadata={'input_tokens': 354, 'output_tokens': 223, 'total_tokens': 577})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(\"what is the recent news in ai?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd64b882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AnyMessage\n",
    "from typing import Annotated\n",
    "from langgraph.graph.message import add_messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2998d64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class state(TypedDict):\n",
    "    messages:Annotated[list[AnyMessage],add_messages]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d351bda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image,display\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13c5b405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x17668f83d70>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Node Defintition\n",
    "def tool_calling_llm(state:state):\n",
    "    return {\"messages\":[llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "# Build Graph\n",
    "builder=StateGraph(state)\n",
    "builder.add_node(\"tool_calling_llm\",tool_calling_llm)\n",
    "builder.add_node(\"tools\",ToolNode(tools))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bd3feb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x17668f83d70>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Edges\n",
    "builder.add_edge(START,\"tool_calling_llm\")\n",
    "builder.add_conditional_edges(\"tool_calling_llm\",tools_condition)\n",
    "builder.add_edge(\"tools\",tool_calling_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ceb27400",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found edge ending at unknown node `<function tool_calling_llm at 0x000001766A4A7F60>`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m graph=\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m display(Image(graph.get_graph().draw_mermaid_png()))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic-Chatbot\\agent\\Lib\\site-packages\\langgraph\\graph\\state.py:829\u001b[39m, in \u001b[36mStateGraph.compile\u001b[39m\u001b[34m(self, checkpointer, cache, store, interrupt_before, interrupt_after, debug, name)\u001b[39m\n\u001b[32m    826\u001b[39m interrupt_after = interrupt_after \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[32m    828\u001b[39m \u001b[38;5;66;03m# validate the graph\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    833\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[38;5;66;03m# prepare output channels\u001b[39;00m\n\u001b[32m    838\u001b[39m output_channels = (\n\u001b[32m    839\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__root__\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    840\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.schemas[\u001b[38;5;28mself\u001b[39m.output_schema]) == \u001b[32m1\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    846\u001b[39m     ]\n\u001b[32m    847\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic-Chatbot\\agent\\Lib\\site-packages\\langgraph\\graph\\state.py:784\u001b[39m, in \u001b[36mStateGraph.validate\u001b[39m\u001b[34m(self, interrupt)\u001b[39m\n\u001b[32m    782\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m all_targets:\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nodes \u001b[38;5;129;01mand\u001b[39;00m target != END:\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound edge ending at unknown node `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    785\u001b[39m \u001b[38;5;66;03m# validate interrupts\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupt:\n",
      "\u001b[31mValueError\u001b[39m: Found edge ending at unknown node `<function tool_calling_llm at 0x000001766A4A7F60>`"
     ]
    }
   ],
   "source": [
    "graph=builder.compile()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a5ca6f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m messages=\u001b[43mgraph\u001b[49m.invoke({\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mgive latest news in ai and then give about attention is all needed research paper\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m      3\u001b[39m     m.pretty_print()\n",
      "\u001b[31mNameError\u001b[39m: name 'graph' is not defined"
     ]
    }
   ],
   "source": [
    "messages=graph.invoke({\"messages\":\"give latest news in ai and then give about attention is all needed research paper\"})\n",
    "for m in messages[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
